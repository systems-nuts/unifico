Swarztrauber() /home/nikos/phd/unified_abi/npb/runs/experiments/performance-regression/o1/10102023/sole/bin/ft_aarch64_aligned.out
Event: cycles

Percent        
               
               
             Disassembly of section .text:
               
             00000000005024a0 <Swarztrauber>:
             Swarztrauber():
             // Swarztrauber to
             // perform FFTs
             //---------------------------------------------------------------------
             static void Swarztrauber(int is, int m, int vlen, int n, int xd1, void *ox,
             dcomplex exponent[n])
             {          
  0.00         sub   sp, sp, #0xc0
               stp   x20, x19, [sp, #160]
               stp   x29, x30, [sp, #176]
  0.00         add   x29, sp, #0xb0
               str   x5, [sp, #8]
               str   w3, [sp, #16]
               str   w1, [sp, #28]
               str   w0, [sp, #84]
               ldr   x8, [x29, #16]
               mov   w19, w4  
               mov   w20, w2  
               str   x8, [sp, #72]
               
             int i, j, l;
             dcomplex u1, x11, x21;
             int k, n1, li, lj, lk, ku, i11, i12, i21, i22;
               
             if (timers_enabled)
               adrp  x8, __libc
               ldr   w8, [x8, #136]
  0.00       ↓ cbz   w8, 48   
             timer_start(4);
               mov   w0, #0x4                        // #4
               nop            
             → bl    timer_start
         48:   ldr   w8, [sp, #16]
               ldr   w3, [sp, #28]
             //---------------------------------------------------------------------
             // Perform one variant of the Stockham FFT.
             //---------------------------------------------------------------------
             n1 = n / 2;
               cmp   w8, #0x0 
               cinc  w8, w8, lt  // lt = tstop
             lj = 1;    
             li = 1 << m;
             for (l = 1; l <= m; l += 2) {
               cmp   w3, #0x1 
             ↓ b.lt  4d0      
               asr   w8, w8, #1
  0.00         str   w8, [sp, #20]
  0.00         ldr   w8, [sp, #28]
  0.00         mov   w4, #0x1                        // #1
               mov   w3, w19  
               mov   w0, #0x1                        // #1
             li = 1 << m;
               lsl   w2, w4, w8
               mov   w8, w20  
               stur  x8, [x29, #-32]
               ldr   w8, [sp, #16]
               mov   w5, w8   
               ldr   x8, [sp, #8]
             for (l = 1; l <= m; l += 2) {
               add   x8, x8, #0x8
               stur  x8, [x29, #-40]
             lk = lj;   
             lj = 2 * lk;
             li = li / 2;
             ku = li;   
               
             for (i = 0; i <= li - 1; i++) {
  0.00         lsl   x8, x3, #4
               str   x5, [sp] 
             ↓ b     c8       
  0.00   a4:   ldr   x3, [sp, #32]
               mov   w30, w3  
  0.00   ac:   ldr   w2, [sp, #28]
             for (l = 1; l <= m; l += 2) {
               add   w4, w4, #0x2
               mov   w3, w4   
  0.00         mov   w0, w30  
  0.02         subs  w3, w3, w2
  0.00         mov   w2, w18  
             ↓ b.gt  4d0      
             lj = 2 * lk;
  0.00   c8:   lsl   w3, w0, #1
             li = li / 2;
  0.00         cmp   w2, #0x0 
               str   x3, [sp, #32]
  0.00         cinc  w3, w2, lt  // lt = tstop
               asr   w18, w3, #1
             for (i = 0; i <= li - 1; i++) {
               cmp   w2, #0x2 
  0.00         str   w4, [sp, #40]
               str   w2, [sp, #24]
               stur  x0, [x29, #-80]
             ↓ b.lt  288      
               sxtw  x3, w18  
               str   x3, [sp, #64]
  0.00         ldr   x3, [sp, #32]
               mov   w4, w0   
               mov   x7, xzr  
  0.00         sxtw  x2, w3   
               sxtw  x3, w0   
               mul   x3, x3, x8
               str   x3, [sp, #48]
  0.00         lsl   w3, w0, #1
               str   w3, [sp, #44]
               mov   w3, #0x210                      // #528
  0.00         mul   x2, x2, x3
               adrp  x3, plane+0x41ea0
               add   x3, x3, #0x168
               str   x2, [sp, #56]
               stur  x3, [x29, #-48]
  0.00         ldr   w3, [sp, #20]
               stp   w0, w3, [x29, #-60]
               mov   w3, w0   
               ldur  x3, [x29, #-40]
               stur  x3, [x29, #-72]
               str   x18, [sp, #88]
             ↓ b     19c      
  0.05  150:   ldr   x3, [sp, #48]
  0.00         ldp   x0, x2, [x29, #-80]
  0.01         add   x7, x7, #0x1
  0.05         add   x2, x2, x3
  0.00         stur  x2, [x29, #-72]
  0.01         ldr   w3, [sp, #44]
  0.00         ldur  w2, [x29, #-60]
  0.04         add   w2, w2, w3
  0.00         ldur  w3, [x29, #-56]
  0.01         add   w3, w3, w0
  0.00         stp   w2, w3, [x29, #-60]
  0.03         ldr   x3, [sp, #56]
  0.00         ldur  x2, [x29, #-48]
  0.01         add   x2, x2, x3
  0.01         stur  x2, [x29, #-48]
  0.04         ldr   x18, [sp, #88]
  0.00         mov   x2, x7   
  0.01         subs  x2, x2, x18
             ↓ b.eq  288      
  0.01  19c:   ldp   x3, x1, [sp, #64]
  0.05         add   x2, x7, x3
  0.01         add   x2, x1, x2, lsl #4
  0.03         ldr   d0, [x2, #8]
  0.00         ldr   w1, [sp, #84]
             i11 = i * lk;
             i12 = i11 + n1;
             i21 = i * lj;
             i22 = i21 + lk;
               
             if (is >= 1) {
  0.25         fneg  d1, d0   
  0.00         cmp   w1, #0x0 
  0.09         fcsel d0, d0, d1, gt
             u1 = exponent[ku + i];
             }          
             else {     
             u1 = dconjg(exponent[ku + i]);
             }          
             for (k = 0; k <= lk - 1; k++) {
  0.00         cmp   w0, #0x1 
  0.00       ↑ b.lt  150      
  0.00         ldr   d1, [x2] 
  0.04         ldur  x18, [x29, #-48]
  0.00         ldp   w1, w30, [x29, #-60]
  0.00         ldur  x0, [x29, #-72]
  0.05         mov   x5, xzr  
             for (j = 0; j < vlen; j++) {
  0.00         cmp   w20, #0x1
             ↓ b.ge  208      
             for (k = 0; k <= lk - 1; k++) {
  0.00  1e0:   add   x5, x5, #0x1
  0.00         mov   x2, x5   
  0.00         add   x0, x0, x8
  0.14         add   w1, w1, #0x1
  0.00         add   w30, w30, #0x1
  0.01         add   x18, x18, #0x210
  0.00         subs  x2, x2, x4
             ↑ b.eq  150      
             for (j = 0; j < vlen; j++) {
  0.12         cmp   w20, #0x1
             ↑ b.lt  1e0      
  0.00  208:   adrp  x3, plane+0x41ea0
  0.00         sxtw  x2, w1   
  0.04         mov   w19, #0x210                     // #528
  0.10         add   x3, x3, #0x160
  0.00         madd  x19, x19, x2, x3
  0.05         add   x17, x19, #0x8
  0.10         ldp   x19, x16, [x29, #-40]
  0.03         sxtw  x2, w30  
  0.04         mov   x6, x18  
  0.10         madd  x2, x2, x8, x19
  0.00         mov   x19, x0  
             x11 = x[i11 + k][j];
  2.95  234:   ldp   d2, d3, [x19, #-8]
             x21 = x[i12 + k][j];
  4.80         ldp   d4, d5, [x2, #-8]
             for (j = 0; j < vlen; j++) {
 10.13         subs  x16, x16, #0x1
  0.46         add   x19, x19, #0x10
  3.50         add   x2, x2, #0x10
             scr[i21 + k][j] = dcmplx_add(x11, x21);
  2.40         fadd  d6, d2, d4
  3.21         fadd  d7, d3, d5
             scr[i22 + k][j] = dcmplx_mul(u1, dcmplx_sub(x11, x21));
  0.47         fsub  d2, d2, d4
  0.13         fsub  d3, d3, d5
  3.32         fmul  d4, d1, d2
  3.48         fmul  d5, d0, d3
  0.63         fmul  d3, d1, d3
  0.00         fmul  d2, d0, d2
  6.03         fsub  d4, d4, d5
  2.87         fadd  d2, d2, d3
             scr[i21 + k][j] = dcmplx_add(x11, x21);
  0.02         stp   d6, d7, [x6, #-8]
             scr[i22 + k][j] = dcmplx_mul(u1, dcmplx_sub(x11, x21));
  0.09         stp   d4, d2, [x17, #-8]
             for (j = 0; j < vlen; j++) {
  9.68         add   x17, x17, #0x10
  0.00         add   x6, x6, #0x10
  0.02       ↑ b.ne  234      
  0.20       ↑ b     1e0      
  0.01  288:   ldr   w4, [sp, #40]
               ldr   w2, [sp, #28]
             }          
             }          
             }          
               
             if (l == m) {
  0.00         mov   w3, w4   
  0.02         subs  w3, w3, w2
  0.00       ↓ b.ne  300      
  0.00         ldr   w3, [sp, #16]
             for (k = 0; k < n; k++) {
  0.00         cmp   w3, #0x1 
             ↑ b.lt  a4       
               ldp   x5, x2, [sp]
  0.00         adrp  x30, plane+0x41ea0
               add   x30, x30, #0x160
               mov   x1, xzr  
             for (j = 0; j < vlen; j++) {
               cmp   w20, #0x1
             ↓ b.ge  2e0      
             for (k = 0; k < n; k++) {
  0.03  2c0:   add   x1, x1, #0x1
               mov   x3, x1   
  0.02         add   x30, x30, #0x210
               add   x2, x2, x8
  0.02         subs  x3, x3, x5
             ↑ b.eq  a4       
             for (j = 0; j < vlen; j++) {
               cmp   w20, #0x1
             ↑ b.lt  2c0      
  0.02  2e0:   ldur  x19, [x29, #-32]
               mov   x0, x2   
  0.02         mov   x3, x30  
             x[k][j] = scr[k][j];
  0.07  2ec:   ldr   q0, [x3], #16
             for (j = 0; j < vlen; j++) {
  0.52         subs  x19, x19, #0x1
             x[k][j] = scr[k][j];
  0.83         str   q0, [x0], #16
             for (j = 0; j < vlen; j++) {
  0.24       ↑ b.ne  2ec      
             ↑ b     2c0      
        300:   ldr   w2, [sp, #24]
             }          
             }          
             }          
             else {     
             lk = lj;   
             lj = 2 * lk;
               lsl   w30, w0, #2
             li = li / 2;
               add   w3, w2, #0x3
               cmp   w2, #0x0 
               csel  w3, w3, w2, lt  // lt = tstop
  0.00         asr   w18, w3, #2
             ku = li;   
               
             for (i = 0; i <= li - 1; i++) {
               cmp   w2, #0x4 
             ↑ b.lt  ac       
               ldr   x3, [sp, #32]
               sxtw  x2, w30  
  0.00         mul   x2, x2, x8
  0.00         lsl   w6, w0, #1
               sxtw  x1, w3   
               sxtw  x3, w18  
               stur  x3, [x29, #-72]
               add   x3, x1, x1, lsl #5
  0.00         lsl   x3, x3, #4
  0.00         str   x3, [sp, #56]
               lsl   w3, w0, #2
               str   w3, [sp, #44]
               adrp  x3, plane+0x41ea0
               str   x30, [sp, #32]
               str   x2, [sp, #64]
               add   x3, x3, #0x168
               stur  x3, [x29, #-48]
  0.00         ldur  x3, [x29, #-40]
               mov   x4, xzr  
               stur  x3, [x29, #-56]
               ldr   w3, [sp, #20]
  0.00         stur  w3, [x29, #-60]
               str   w6, [sp, #48]
               str   x18, [sp, #88]
             ↓ b     3d4      
  0.04  384:   ldr   x3, [sp, #56]
  0.01         ldur  x2, [x29, #-48]
  0.01         ldur  x0, [x29, #-80]
  0.01         add   x4, x4, #0x1
  0.03         add   x2, x2, x3
  0.01         stur  x2, [x29, #-48]
  0.00         ldr   w3, [sp, #44]
  0.01         ldur  w2, [x29, #-60]
  0.00         add   w6, w6, w3
  0.02         ldr   w3, [sp, #48]
  0.00         add   w2, w2, w3
  0.00         stur  w2, [x29, #-60]
  0.00         ldr   x3, [sp, #64]
  0.01         ldur  x2, [x29, #-56]
  0.00         add   x2, x2, x3
  0.01         stur  x2, [x29, #-56]
  0.00         ldr   x18, [sp, #88]
  0.01         mov   x3, x4   
  0.00         subs  x3, x3, x18
  0.01       ↓ b.eq  4c4      
  0.00  3d4:   ldur  x3, [x29, #-72]
  0.01         ldr   x2, [sp, #72]
  0.00         add   x3, x4, x3
  0.01         add   x3, x2, x3, lsl #4
  0.01         ldr   d0, [x3, #8]
  0.00         ldr   w2, [sp, #84]
             i11 = i * lk;
             i12 = i11 + n1;
             i21 = i * lj;
             i22 = i21 + lk;
               
             if (is >= 1) {
  0.19         fneg  d1, d0   
               cmp   w2, #0x0 
  0.04         fcsel d0, d0, d1, gt
             u1 = exponent[ku + i];
             }          
             else {     
             u1 = dconjg(exponent[ku + i]);
             }          
             for (k = 0; k <= lk - 1; k++) {
  0.00         cmp   w0, #0x1 
             ↑ b.lt  384      
  0.00         ldr   d1, [x3] 
  0.00         ldp   x30, x7, [x29, #-56]
  0.38         ldur  w16, [x29, #-60]
               mov   w18, w6  
               mov   x3, xzr  
             for (j = 0; j < vlen; j++) {
  0.02         cmp   w20, #0x1
             ↓ b.ge  444      
             for (k = 0; k <= lk - 1; k++) {
  0.09  41c:   add   x3, x3, #0x1
  0.02         mov   x2, x3   
  0.02         add   x7, x7, #0x210
  0.00         add   w18, w18, #0x1
  0.08         add   w16, w16, #0x1
  0.02         add   x30, x30, x8
  0.01         subs  x2, x2, x1
             ↑ b.ge  384      
             for (j = 0; j < vlen; j++) {
  0.01         cmp   w20, #0x1
  0.08       ↑ b.lt  41c      
  0.02  444:   ldp   x2, x17, [x29, #-40]
  0.06         sxtw  x5, w18  
  0.10         adrp  x19, plane+0x41ea0
  0.02         mov   w0, #0x210                      // #528
  0.01         madd  x5, x5, x8, x2
  0.08         sxtw  x2, w16  
  0.02         add   x19, x19, #0x160
  0.01         madd  x0, x0, x2, x19
  0.10         add   x2, x0, #0x8
  0.02         mov   x19, x30 
  0.01         mov   x0, x7   
             x11 = scr[i11 + k][j];
  0.10  470:   ldp   d2, d3, [x0, #-8]
             x21 = scr[i12 + k][j];
  2.87         ldp   d4, d5, [x2, #-8]
             for (j = 0; j < vlen; j++) {
  3.09         subs  x17, x17, #0x1
  2.28         add   x0, x0, #0x10
  2.92         add   x2, x2, #0x10
             x[i21 + k][j] = dcmplx_add(x11, x21);
  1.84         fadd  d6, d2, d4
  0.52         fadd  d7, d3, d5
             x[i22 + k][j] = dcmplx_mul(u1, dcmplx_sub(x11, x21));
  2.18         fsub  d2, d2, d4
  0.63         fsub  d3, d3, d5
  2.95         fmul  d4, d1, d2
  0.91         fmul  d5, d0, d3
  2.01         fmul  d3, d1, d3
  0.50         fmul  d2, d0, d2
  6.06         fsub  d4, d4, d5
  0.92         fadd  d2, d2, d3
             x[i21 + k][j] = dcmplx_add(x11, x21);
  0.94         stp   d6, d7, [x19, #-8]
             x[i22 + k][j] = dcmplx_mul(u1, dcmplx_sub(x11, x21));
  0.65         stp   d4, d2, [x5, #-8]
             for (j = 0; j < vlen; j++) {
  8.85         add   x5, x5, #0x10
  0.33         add   x19, x19, #0x10
             ↑ b.ne  470      
  0.00       ↑ b     41c      
  0.01  4c4:   ldr   w4, [sp, #40]
  0.00         ldr   x30, [sp, #32]
             ↑ b     ac       
             }          
             }          
             }          
             }          
             }          
             if (timers_enabled)
  0.00  4d0:   adrp  x8, __libc
               ldr   w8, [x8, #136]
  0.01       ↓ cbz   w8, 4f0  
             timer_stop(4);
               ldp   x29, x30, [sp, #176]
               ldp   x20, x19, [sp, #160]
               mov   w0, #0x4                        // #4
               add   sp, sp, #0xc0
             → b     timer_stop
             }          
        4f0:   ldp   x29, x30, [sp, #176]
  0.00         ldp   x20, x19, [sp, #160]
  0.00         add   sp, sp, #0xc0
             ← ret            
